{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG using Opensource local-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Using local model for improving latency)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "model_id = \"meta/llama3-70b-instruct\"\n",
    "llm = ChatNVIDIA(model=model_id) # llm\n",
    "embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\") #nvidia embeddings\n",
    "vector_store = Chroma(embedding_function=embeddings) # chroma local vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m \"punderful\" today, thanks for asking! I\\'m \"egg-static\" to be of assistance. Don\\'t worry, I won\\'t \"leaf\" you hanging ‚Äì I\\'m \"rooting\" for our conversation to be a-maize-ing!', additional_kwargs={}, response_metadata={'role': 'assistant', 'content': 'I\\'m \"punderful\" today, thanks for asking! I\\'m \"egg-static\" to be of assistance. Don\\'t worry, I won\\'t \"leaf\" you hanging ‚Äì I\\'m \"rooting\" for our conversation to be a-maize-ing!', 'token_usage': {'prompt_tokens': 36, 'total_tokens': 91, 'completion_tokens': 55}, 'finish_reason': 'stop', 'model_name': 'meta/llama3-70b-instruct'}, id='run-2acc1113-e878-43a8-beb2-796da779ae86-0', usage_metadata={'input_tokens': 36, 'output_tokens': 55, 'total_tokens': 91}, role='assistant')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## üòÜtesting\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful comedic assistant that answers only using puns.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The app flow would be like this --\n",
    "# 1. It receive some question from the user \n",
    "# 2. It creates query from that question (QueryGenerator)\n",
    "# 3. Using that query it retrieves the required document and create the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER MESSAGES for testing the app-flow\n",
    "um1 = HumanMessage('hello chat')\n",
    "um2 = HumanMessage('what is self control?')\n",
    "um3 = HumanMessage('how to use it?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Query Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "QueryMessages = [\n",
    "    SystemMessage(content='''You are a query generator. You will be provided with the chat of the human.\n",
    "                  According to that chat you return a query for searching in the vector database.\n",
    "                  The query should be for finding relevant context for the last question.\n",
    "                  Just return the query only in the format \"query : []\"\n",
    "                  If you dont think a relevant query exists return an empty array otherwise return at max 1 query.\n",
    "                  '''),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='query : []', additional_kwargs={}, response_metadata={'role': 'assistant', 'content': 'query : []', 'token_usage': {'prompt_tokens': 99, 'total_tokens': 103, 'completion_tokens': 4}, 'finish_reason': 'stop', 'model_name': 'meta/llama3-70b-instruct'}, id='run-219ff442-da65-484b-9913-07739f757eb3-0', usage_metadata={'input_tokens': 99, 'output_tokens': 4, 'total_tokens': 103}, role='assistant')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryMessages += [um1]\n",
    "llm.invoke(QueryMessages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='query : [\"definition of self control\"]', additional_kwargs={}, response_metadata={'role': 'assistant', 'content': 'query : [\"definition of self control\"]', 'token_usage': {'prompt_tokens': 109, 'total_tokens': 118, 'completion_tokens': 9}, 'finish_reason': 'stop', 'model_name': 'meta/llama3-70b-instruct'}, id='run-b8f006ce-d5e8-4d6b-98e4-4796db59ffab-0', usage_metadata={'input_tokens': 109, 'output_tokens': 9, 'total_tokens': 118}, role='assistant')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryMessages += [um2]\n",
    "llm.invoke(QueryMessages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'query : [\"techniques for self control\"]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryMessages += [um3]\n",
    "llm.invoke(QueryMessages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a query generator. You will be provided with the chat of the human.\\n                  According to that chat you return a query for searching in the vector database.\\n                  The query should be for finding relevant context for the last question.\\n                  Just return the query only in the format \"query : []\"\\n                  If you dont think a relevant query exists return an empty array otherwise return at max 1 query.\\n                  ', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hello chat', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is self control?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='how to use it?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryMessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_generator(QueryMsgs):\n",
    "    qs = llm.invoke(QueryMsgs).content\n",
    "    return qs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = (\n",
    "    \"../docs/self-mastery.pdf\"\n",
    ")\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "document = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunked_documents = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7bfac579-6199-4010-b12c-d91e3c1c2e79',\n",
       " 'ef42cf2f-650b-4992-ad54-c0337bdeea26',\n",
       " 'bde03a4c-a17f-40b1-9e25-ad6190889dd7']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = vector_store.add_documents(chunked_documents)\n",
    "_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(queries):\n",
    "    retrieved_docs = []\n",
    "    for query in queries : \n",
    "        retrieved_docs += [vector_store.similarity_search(query)]\n",
    "        \n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'page': 66, 'source': '../docs/self-mastery.pdf'}, page_content='thinking God is going to control your life and harm you. He loves you! Once you \\nsurrender, you will be forgiven and blessed beyond imagination. Your addiction will \\nDISSAPEAR into the DUST!'),\n",
       "  Document(metadata={'page': 101, 'source': '../docs/self-mastery.pdf'}, page_content='5. Watch a Wim Hoff video. \\n6. Get out and be cold. \\n7. The key is to humbly submit to God‚Äôs grace with the help of the pain. \\n8. The idea is not to punish yourself but to grow through discipline, self-\\ncare, and love for your life.'),\n",
       "  Document(metadata={'page': 102, 'source': '../docs/self-mastery.pdf'}, page_content='and drink, but  putting off all impurities, like gossiping,  lust, backbiting, foul \\nlanguage, and all other sins and immoralities. It is a full cleanse of the being  ‚Äì \\nthe best medicine: \\n\"Fasting is the first principle of medicine. Fast and see                                    \\nthe strength of the spirit reveal itself.\"                                                                   \\n‚Äì Rumi \\n. . . \\n \\nTypes of Fasting \\n \\n(consult a medical professional if you have never fasted before) \\nDry Fasting: This is when you go without food or water for a specific time. \\nIntermittent Fasting: This is when you go without food , but can only drink \\nwater for a specific time.'),\n",
       "  Document(metadata={'page': 22, 'source': '../docs/self-mastery.pdf'}, page_content='‚Äì Mahatma Gandhi \\n \\nYou must t ake powerful steps towards purity in order to contain it and never \\nput yourself in a position to allow it  to spill. For example, even by engaging in \\nsomething lustful , like peeking at a sexual image, your body will release this \\nfluid prematurely.  \\nThis is a no-go! You must stay pure, so it goes up your spine rather than down \\nand out. Purity is power. If I can get you to drain it. I‚Äôve finished you. There is \\nno battle you could fight against me. If I can get you to drain your semen \\ncompletely, you‚Äôre finished as a person: \\n \\n‚ÄúAsk Muhammed Ali! ‚Äì If he ejaculates, he can‚Äôt fight                                        \\ntwo minutes. Sh*t, he couldn‚Äôt even whip me‚Ä¶                                          \\nYou give up all of your energy when you                                                                \\nejaculate. I mean, you give up all of it!‚Äù                                                                    \\n‚Äì Miles Davis')]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever(['self discipline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! TODO: Create the final generator using the above 2 components .... right now going to build the final app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
